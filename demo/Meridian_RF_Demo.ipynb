{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daeexe/Meridian/blob/dev/demo/Meridian_RF_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuQtvbG_vILv"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/meridian/blob/main/demo/Meridian_RF_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/meridian/blob/main/demo/Meridian_RF_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqSiFABximWU"
      },
      "source": [
        "# **Meridian Reach and Frequency Demo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckR-pavwis-Q"
      },
      "source": [
        "Welcome to the Meridian end-to-end demo for Reach and Frequency data. This simplified demo showcases the fundamental functionalities and basic usage of the library on data containing Reach and Frequency channels, including working examples of the major modeling steps:\n",
        "\n",
        "\n",
        "<ol start=\"0\">\n",
        "  <li><a href=\"#install\">Install</a></li>\n",
        "  <li><a href=\"#load-data\">Load the data</a></li>\n",
        "  <li><a href=\"#configure-model\">Configure the model</a></li>\n",
        "  <li><a href=\"#model-diagnostics\">Run model diagnostics</a></li>\n",
        "  <li><a href=\"#generate-summary\">Generate model results & two-page output</a></li>\n",
        "  <li><a href=\"#generate-optimize\">Run budget optimization & two-page output</a></li>\n",
        "  <li><a href=\"#save-model\">Save the model object</a></li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "Note that this notebook skips all of the exploratory data analysis and preprocessing steps. It assumes that you have completed these tasks before reaching this point in the demo.\n",
        "\n",
        "This notebook utilizes sample data. As a result, the numbers and results obtained might not accurately reflect what you encounter when working with a real dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GicRPam0mUhF"
      },
      "source": [
        "<a name=\"install\"></a>\n",
        "## Step 0: Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDdX9WofM2fx"
      },
      "source": [
        "1\\. Make sure you are using one of the available GPU Colab runtimes which is **required** to run Meridian. You can change your notebook's runtime in `Runtime > Change runtime type` in the menu. All users can use the T4 GPU runtime which is sufficient to run the demo colab, free of charge. Users who have purchased one of Colab's paid plans have access to premium GPUs (such as V100, A100 or L4 Nvidia GPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFYRTDuesa1P"
      },
      "source": [
        "2\\. Install the latest version of Meridian, and verify that GPU is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1jAk386jF3k"
      },
      "outputs": [],
      "source": [
        "# Install meridian: from PyPI @ latest release\n",
        "!pip install --upgrade google-meridian[colab,and-cuda]\n",
        "\n",
        "# Install meridian: from PyPI @ specific version\n",
        "# !pip install google-meridian[colab,and-cuda]==1.0.3\n",
        "\n",
        "# Install meridian: from GitHub @HEAD\n",
        "# !pip install --upgrade \"google-meridian[colab,and-cuda] @ git+https://github.com/google/meridian.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhwt1wzgLwpZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import arviz as az\n",
        "\n",
        "import IPython\n",
        "\n",
        "from meridian import constants\n",
        "from meridian.data import load\n",
        "from meridian.data import test_utils\n",
        "from meridian.model import model\n",
        "from meridian.model import spec\n",
        "from meridian.model import prior_distribution\n",
        "from meridian.analysis import optimizer\n",
        "from meridian.analysis import analyzer\n",
        "from meridian.analysis import visualizer\n",
        "from meridian.analysis import summarizer\n",
        "from meridian.analysis import formatter\n",
        "\n",
        "# check if GPU is available\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiM0UrN6qbIP"
      },
      "source": [
        "<a name=\"load-data\"></a>\n",
        "## Step 1: Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z18Mo-22x0lY"
      },
      "source": [
        "Load the [simulated dataset in CSV format](https://github.com/google/meridian/blob/main/meridian/data/simulated_data/csv/geo_media_rf.csv) as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZd-ik8NbjK6"
      },
      "source": [
        "1\\. Map the column names to their corresponding variable types. For example, the column names 'GQV' and 'Competitor_Sales' are mapped to `controls`. The required variable types are `time`, `controls`, `population`, `kpi`, `revenue_per_kpi`, `media` and `spend`. If your data includes organic media or non-media treatments, you can add them using `organic_media` and `non_media_treatments` arguments. For the definition of each variable, see\n",
        "[Collect and organize your data](https://developers.google.com/meridian/docs/user-guide/collect-data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sV1ChiEYuyD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import the library to mount Google Drive\n",
        "from google.colab import drive\n",
        "# Mount the Google Drive at /content/drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbHkSEIkbQQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Load the CSV file\n",
        "file_path = \"/content/drive/MyDrive/MMM_Meridian.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the raw column names to diagnose the issue\n",
        "print(\"Raw column names:\", df.columns.tolist())\n",
        "\n",
        "# 2. Clean column names (remove spaces and dots)\n",
        "# Let's be more robust in cleaning, including newlines and multiple spaces\n",
        "df.columns = df.columns.str.strip()                      # Remove extra spaces\n",
        "df.columns = df.columns.str.replace('\\n', ' ', regex=False) # Replace newlines with spaces\n",
        "df.columns = df.columns.str.replace('.', '', regex=False)  # Remove periods\n",
        "df.columns = df.columns.str.replace(' ', '_')            # Replace spaces with underscores\n",
        "\n",
        "# Print cleaned column names\n",
        "print(\"Cleaned column names:\", df.columns.tolist())\n",
        "\n",
        "\n",
        "# 3. Clean and convert numeric columns\n",
        "# Use try-except blocks or check if columns exist before processing\n",
        "# Based on the printed cleaned names, the numeric columns are campaign-specific\n",
        "# and also 'Population', 'Monetary_Value', 'Brand_GQV', 'Generic_GQV', 'SOV', 'Total_Conversions'\n",
        "numeric_cols_to_clean = [\n",
        "    'Population', 'Monetary_Value', 'Brand_GQV', 'Generic_GQV', 'SOV', 'Total_Conversions'\n",
        "]\n",
        "\n",
        "# Add campaign-specific metric columns dynamically\n",
        "campaign_types_in_data = ['Display', 'Display_Plug', 'Display_STB', 'Facebook', 'Facebook_Awareness', 'Facebook_Consideration', 'Facebook_Conversion', 'Facebook_Share_to_Buy', 'Google_Demand_Gen', 'Google_Performance_Max', 'Google_Search', 'TikTok', 'YouTube'] # Based on cleaned column names\n",
        "metrics_in_data = ['Impressions', 'Media_Cost', 'Total_Conversions', 'Total_Conversions_CPA'] # Based on cleaned column names\n",
        "\n",
        "for campaign in campaign_types_in_data:\n",
        "    for metric in metrics_in_data:\n",
        "        col_name = f\"{campaign}_{metric}\"\n",
        "        if col_name in df.columns:\n",
        "            numeric_cols_to_clean.append(col_name)\n",
        "\n",
        "# Remove duplicates and ensure columns exist\n",
        "numeric_cols_to_clean = list(set(numeric_cols_to_clean))\n",
        "numeric_cols_to_clean = [col for col in numeric_cols_to_clean if col in df.columns]\n",
        "\n",
        "print(\"Numeric columns to clean:\", numeric_cols_to_clean)\n",
        "\n",
        "for col in numeric_cols_to_clean:\n",
        "    try:\n",
        "        df[col] = df[col].astype(str).str.replace(',', '', regex=False) # Remove commas\n",
        "        df[col] = df[col].replace('--', pd.NA) # Handle '--' or other non-numeric strings\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')    # Convert to numeric, turn errors into NaN\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process column {col}: {e}\")\n",
        "\n",
        "\n",
        "# Clean and convert the date column\n",
        "# Based on the printed raw column names, the date column is 'Week Start Date'\n",
        "date_column_name_raw = 'Week Start Date'\n",
        "date_column_name_cleaned = 'Week_Start_Date' # After cleaning\n",
        "\n",
        "date_col_found = False\n",
        "if date_column_name_cleaned in df.columns:\n",
        "    try:\n",
        "        df[date_column_name_cleaned] = pd.to_datetime(df[date_column_name_cleaned], errors='coerce')\n",
        "        # Rename to 'Week' as expected by Meridian\n",
        "        df = df.rename(columns={date_column_name_cleaned: 'Week'})\n",
        "        date_col_found = True\n",
        "        print(f\"Successfully processed date column: {date_column_name_cleaned}\")\n",
        "    except Exception as e:\n",
        "         print(f\"Could not process date column {date_column_name_cleaned}: {e}\")\n",
        "elif date_column_name_raw in df.columns:\n",
        "     # This case should not happen if cleaning works, but as a fallback\n",
        "     try:\n",
        "        df[date_column_name_raw] = pd.to_datetime(df[date_column_name_raw], errors='coerce')\n",
        "        df = df.rename(columns={date_column_name_raw: 'Week'})\n",
        "         # Check if the cleaned name is now in columns after renaming from raw\n",
        "        if 'Week_Start_Date' in df.columns:\n",
        "            df = df.rename(columns={'Week_Start_Date': 'Week'})\n",
        "        date_col_found = True\n",
        "        print(f\"Successfully processed date column: {date_column_name_raw}\")\n",
        "     except Exception as e:\n",
        "         print(f\"Could not process date column {date_column_name_raw}: {e}\")\n",
        "\n",
        "\n",
        "if not date_col_found:\n",
        "    print(f\"Could not find or process date column. Looked for '{date_column_name_raw}' and '{date_column_name_cleaned}'.\")\n",
        "\n",
        "\n",
        "# Assuming 'Region' needs to be renamed to 'Region_(Matched)'\n",
        "region_column_name_raw = 'Region'\n",
        "region_column_name_cleaned = 'Region' # After cleaning, if no spaces\n",
        "region_target_name = 'Region_(Matched)'\n",
        "\n",
        "if region_column_name_cleaned in df.columns:\n",
        "    if region_column_name_cleaned != region_target_name:\n",
        "        df = df.rename(columns={region_column_name_cleaned: region_target_name})\n",
        "    print(f\"Successfully processed region column: {region_column_name_cleaned}\")\n",
        "elif region_column_name_raw in df.columns:\n",
        "     if region_column_name_raw != region_target_name:\n",
        "        df = df.rename(columns={region_column_name_raw: region_target_name})\n",
        "     print(f\"Successfully processed region column: {region_column_name_raw}\")\n",
        "else:\n",
        "    print(f\"Could not find region column. Looked for '{region_column_name_raw}' and '{region_column_name_cleaned}'.\")\n",
        "\n",
        "\n",
        "# Assuming 'Campaign_type' is not a column to pivot on based on the wide format\n",
        "if 'Campaign_type' in df.columns:\n",
        "    print(\"Warning: 'Campaign_type' column found but not used for pivoting based on wide data format.\")\n",
        "    # You might want to drop it or use it for filtering if needed\n",
        "\n",
        "\n",
        "# 4. Select needed columns for the pivot_table\n",
        "# Include 'Week', 'Region_(Matched)', and all cleaned numeric columns\n",
        "selected_columns = ['Week', 'Region_(Matched)'] + numeric_cols_to_clean\n",
        "\n",
        "# Ensure all selected columns actually exist in the DataFrame after renaming\n",
        "selected_columns = [col for col in selected_columns if col in df.columns]\n",
        "print(\"Selected columns for pivot_table:\", selected_columns)\n",
        "\n",
        "# Ensure 'Week' and 'Region_(Matched)' are in the selected columns for grouping\n",
        "if 'Week' not in selected_columns:\n",
        "     print(\"Error: 'Week' column is missing after processing.\")\n",
        "if 'Region_(Matched)' not in selected_columns:\n",
        "     print(\"Error: 'Region_(Matched)' column is missing after processing.\")\n",
        "\n",
        "\n",
        "df_subset = df[selected_columns].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "# 5. Group by Week and Region_(Matched) and sum the relevant columns\n",
        "# This step is still useful to aggregate data if there are multiple rows for the same week/region combination\n",
        "# or just to ensure the expected structure.\n",
        "# Identify columns to sum (all columns except 'Week' and 'Region_(Matched)')\n",
        "columns_to_sum = [col for col in df_subset.columns if col not in ['Week', 'Region_(Matched)']]\n",
        "\n",
        "if 'Week' in df_subset.columns and 'Region_(Matched)' in df_subset.columns:\n",
        "    grouped_df = df_subset.groupby(['Week', 'Region_(Matched)'])[columns_to_sum].sum().reset_index()\n",
        "    print(\"\\nGrouped data info:\")\n",
        "    grouped_df.info()\n",
        "else:\n",
        "    print(\"\\nSkipping grouping step as 'Week' or 'Region_(Matched)' is missing.\")\n",
        "    grouped_df = df_subset.copy() # If grouping keys are missing, just use the subset as is\n",
        "\n",
        "\n",
        "# 6. Rename columns to match the expected format for Meridian (e.g., 'ChannelName_Impression', 'ChannelName_Cost')\n",
        "# Based on the Meridian example and the cleaned column names, we need to map\n",
        "# names like 'Google_Performance_Max_Impressions' to 'Performance_Max_Impression'.\n",
        "# This requires identifying the channel name and the metric name from the cleaned column names.\n",
        "\n",
        "# Let's redefine the mapping based on the cleaned column names\n",
        "# Example: 'Google_Performance_Max_Impressions' -> Channel: 'Performance_Max', Metric: 'Impression'\n",
        "# 'Display_Media_Cost' -> Channel: 'Display', Metric: 'Cost'\n",
        "# 'Facebook_Total_Conversions' -> Channel: 'Facebook', Metric: 'Conversions'\n",
        "\n",
        "# Identify potential channel prefixes and metric suffixes from cleaned column names\n",
        "channel_prefixes = list(set([col.split('_')[0] for col in grouped_df.columns if '_' in col]))\n",
        "metric_suffixes = list(set([col.split('_')[-1] for col in grouped_df.columns if '_' in col]))\n",
        "\n",
        "print(\"\\nIdentified potential channel prefixes:\", channel_prefixes)\n",
        "print(\"Identified potential metric suffixes:\", metric_suffixes)\n",
        "\n",
        "# Create a more robust renaming logic\n",
        "rename_map = {}\n",
        "# Iterate through all columns in the grouped_df\n",
        "for col in grouped_df.columns:\n",
        "    if col in ['Week', 'Region_(Matched)', 'Population', 'Monetary_Value', 'Brand_GQV', 'Generic_GQV', 'SOV']:\n",
        "        # Keep non-media columns as they are or rename specifically if needed\n",
        "        # Based on coord_to_columns, 'Monetary_Value' should be 'Revenue'\n",
        "        if col == 'Monetary_Value':\n",
        "            rename_map[col] = 'Revenue'\n",
        "        # 'Brand_GQV' and 'Generic_GQV' should be in 'controls'\n",
        "        # 'SOV' could also be a control\n",
        "        # 'Population' is mapped correctly\n",
        "        # 'Total_Conversions' is the KPI\n",
        "        elif col == 'Total_Conversions':\n",
        "             rename_map[col] = 'Total_Conversions' # Keep as is for now, will be mapped to kpi later\n",
        "        else:\n",
        "            rename_map[col] = col # Keep other non-media columns as is\n",
        "\n",
        "    # Check for campaign_type_metric pattern\n",
        "    # Look for patterns like ChannelName_MetricName (e.g., Display_Impressions)\n",
        "    # Or potentially ChannelName_SubChannel_MetricName (e.g., Facebook_Awareness_Impressions)\n",
        "    parts = col.split('_')\n",
        "    if len(parts) >= 2:\n",
        "        metric_part = parts[-1]\n",
        "        channel_part = '_'.join(parts[:-1]) # Join all parts except the last one as channel\n",
        "\n",
        "        # Map the metric part to the desired Meridian metric name\n",
        "        meridian_metric_name = None\n",
        "        if metric_part == 'Impressions':\n",
        "            meridian_metric_name = 'Impression'\n",
        "        elif metric_part == 'Media_Cost':\n",
        "            meridian_metric_name = 'Cost'\n",
        "        elif metric_part == 'Total_Conversions':\n",
        "             meridian_metric_name = 'Conversions'\n",
        "\n",
        "        if meridian_metric_name:\n",
        "            # Map the channel part to the desired Meridian channel name\n",
        "            # This might require a specific mapping if the cleaned channel names\n",
        "            # don't directly match the desired channel names in correct_media_to_channel\n",
        "            # For now, let's use the cleaned channel part as the channel name\n",
        "            meridian_channel_name = channel_part\n",
        "\n",
        "            # Adjust channel names to match the expected format in correct_media_to_channel/spend_to_channel\n",
        "            # Examples from correct_media_to_channel/spend_to_channel: 'Search', 'Performance_Max', 'Display', 'Demand_Gen'\n",
        "            # The cleaned data has: 'Google_Search', 'Google_Performance_Max', 'Display', 'Google_Demand_Gen', 'Facebook', 'TikTok', 'YouTube', etc.\n",
        "            # We need to map the cleaned names to the desired channel names.\n",
        "\n",
        "            # Let's create a mapping based on the observed cleaned names and the desired Meridian channel names\n",
        "            cleaned_to_meridian_channel_map = {\n",
        "                'Google_Search': 'Search',\n",
        "                'Google_Performance_Max': 'Performance_Max',\n",
        "                'Display': 'Display',\n",
        "                'Google_Demand_Gen': 'Demand_Gen',\n",
        "                'Facebook': 'Facebook',\n",
        "                'TikTok': 'TikTok',\n",
        "                'YouTube': 'YouTube',\n",
        "                'Display_Plug': 'Display_Plug', # Example: if you want to keep sub-channels separate\n",
        "                'Display_STB': 'Display_STB',\n",
        "                'Facebook_Awareness': 'Facebook_Awareness',\n",
        "                'Facebook_Consideration': 'Facebook_Consideration',\n",
        "                'Facebook_Conversion': 'Facebook_Conversion',\n",
        "                'Facebook_Share_to_Buy': 'Facebook_Share_to_Buy',\n",
        "                # Add other mappings as needed based on your data and desired channels\n",
        "            }\n",
        "\n",
        "            mapped_channel_name = cleaned_to_meridian_channel_map.get(meridian_channel_name, meridian_channel_name) # Use cleaned name if not in map\n",
        "\n",
        "            new_col_name = f\"{mapped_channel_name}_{meridian_metric_name}\"\n",
        "            rename_map[col] = new_col_name\n",
        "        else:\n",
        "             # If the metric part doesn't match expected metrics, keep the original name\n",
        "             rename_map[col] = col\n",
        "\n",
        "\n",
        "print(\"\\nRename map generated:\", rename_map)\n",
        "pivot_table = grouped_df.rename(columns=rename_map)\n",
        "\n",
        "\n",
        "# Ensure 'Total_Conversions' is present and named correctly for KPI\n",
        "if 'Total_Conversions' not in pivot_table.columns:\n",
        "     print(\"Warning: 'Total_Conversions' column is missing after renaming.\")\n",
        "     # Attempt to create it by summing if campaign conversions exist\n",
        "     conversion_cols = [col for col in pivot_table.columns if col.endswith('_Conversions')]\n",
        "     if conversion_cols:\n",
        "         pivot_table['Total_Conversions'] = pivot_table[conversion_cols].sum()\n",
        "         print(\"Created 'Total_Conversions' by summing campaign conversions.\")\n",
        "     else:\n",
        "         print(\"Error: Could not create 'Total_Conversions'. No columns ending with '_Conversions' found.\")\n",
        "         pivot_table['Total_Conversions'] = 0 # Add a placeholder if no conversion columns are found\n",
        "\n",
        "# Explicitly rename 'Monetary_Value' to 'Revenue' if it exists\n",
        "if 'Monetary_Value' in pivot_table.columns:\n",
        "    pivot_table = pivot_table.rename(columns={'Monetary_Value': 'Revenue'})\n",
        "    print(\"Renamed 'Monetary_Value' to 'Revenue'.\")\n",
        "\n",
        "\n",
        "# 7. Ensure complete time series and region combinations (This part is still relevant)\n",
        "\n",
        "# Get all unique regions from the grouped data\n",
        "if 'Region_(Matched)' in pivot_table.columns:\n",
        "    all_regions = pivot_table['Region_(Matched)'].unique()\n",
        "else:\n",
        "    print(\"Error: 'Region_(Matched)' column not found for generating full grid.\")\n",
        "    all_regions = [] # Cannot proceed with full grid without region\n",
        "\n",
        "# Get the min and max dates to create a full date range from the grouped data\n",
        "if 'Week' in pivot_table.columns:\n",
        "    min_date = pivot_table['Week'].min()\n",
        "    max_date = pivot_table['Week'].max()\n",
        "    # Create a complete weekly date range\n",
        "    full_date_range = pd.date_range(start=min_date, end=max_date, freq='W-MON') # Assuming weekly data starts on Monday\n",
        "    print(f\"\\nGenerated full date range from {min_date} to {max_date}\")\n",
        "else:\n",
        "    print(\"\\nError: 'Week' column not found for generating full date range.\")\n",
        "    full_date_range = pd.to_datetime([]) # Empty date range\n",
        "\n",
        "# Create a complete grid of all week and region combinations\n",
        "if not full_date_range.empty and all_regions.size > 0:\n",
        "    full_grid = pd.MultiIndex.from_product([full_date_range, all_regions], names=['Week', 'Region_(Matched)']).to_frame(index=False)\n",
        "    print(\"Generated full grid of week and region combinations.\")\n",
        "\n",
        "    # Convert 'Week' in pivot_table to datetime for merging if it's not already\n",
        "    pivot_table['Week'] = pd.to_datetime(pivot_table['Week'])\n",
        "\n",
        "    # Merge the combined data onto the full grid\n",
        "    # Use a left merge to keep all combinations from the full_grid\n",
        "    pivot_table = full_grid.merge(pivot_table, on=['Week', 'Region_(Matched)'], how='left')\n",
        "    print(\"Merged data onto full grid.\")\n",
        "else:\n",
        "    print(\"Skipping full grid merge due to missing 'Week' or 'Region_(Matched)' column.\")\n",
        "\n",
        "\n",
        "# 8. Fill NaN values after ensuring all week/region combinations exist\n",
        "# Identify columns that should be filled with 0 after merge\n",
        "# These are the metric columns (all columns except 'Week' and 'Region_(Matched)')\n",
        "zero_fill_columns = [col for col in pivot_table.columns if col not in ['Week', 'Region_(Matched)']]\n",
        "\n",
        "for col in zero_fill_columns:\n",
        "    if col in pivot_table.columns:\n",
        "        pivot_table[col] = pivot_table[col].fillna(0)\n",
        "print(\"Filled NaN values with 0 for metric columns.\")\n",
        "\n",
        "\n",
        "# Convert 'Week' back to string format expected by Meridian\n",
        "if 'Week' in pivot_table.columns:\n",
        "    pivot_table['Week'] = pivot_table['Week'].dt.strftime('%Y-%m-%d')\n",
        "    print(\"Converted 'Week' column back to string format.\")\n",
        "else:\n",
        "    print(\"Error: 'Week' column not found after filling NaNs.\")\n",
        "\n",
        "\n",
        "# 9. Final reorder (Adjust column ordering based on available columns)\n",
        "# Identify columns dynamically based on the final pivot_table\n",
        "all_cols = pivot_table.columns.tolist()\n",
        "fixed_cols = ['Week', 'Region_(Matched)']\n",
        "\n",
        "# Separate media and non-media metrics for ordering\n",
        "# Media columns end with '_Impression' or '_Cost'\n",
        "media_cols = sorted([col for col in all_cols if col.endswith('_Impression') or col.endswith('_Cost')])\n",
        "# Non-media columns are everything else except the fixed columns and Total_Conversions and Revenue\n",
        "non_media_cols = sorted([col for col in all_cols if col not in fixed_cols and col not in media_cols and col != 'Total_Conversions' and col != 'Revenue'])\n",
        "\n",
        "ordered_columns = fixed_cols + media_cols + non_media_cols + ['Total_Conversions', 'Revenue']\n",
        "\n",
        "# Ensure all ordered columns are actually in the dataframe before reordering\n",
        "ordered_columns = [col for col in ordered_columns if col in pivot_table.columns]\n",
        "pivot_table = pivot_table[ordered_columns]\n",
        "print(\"\\nReordered columns.\")\n",
        "\n",
        "\n",
        "# 10. Preview\n",
        "print(\"\\nPivoted and cleaned data preview:\")\n",
        "print(pivot_table.head())\n",
        "print(\"\\nData Info:\")\n",
        "pivot_table.info()\n",
        "\n",
        "if 'Week' in pivot_table.columns:\n",
        "    print(f\"\\n{pivot_table['Week'].nunique()} unique weeks\")\n",
        "else:\n",
        "    print(\"\\n'Week' column not found in final pivot_table.\")\n",
        "\n",
        "if 'Region_(Matched)' in pivot_table.columns:\n",
        "    print(f\"{pivot_table['Region_(Matched)'].nunique()} unique regions\")\n",
        "else:\n",
        "    print(\"'Region_(Matched)' column not found in final pivot_table.\")\n",
        "\n",
        "print(f\"{pivot_table.shape[0]} rows, {pivot_table.shape[1]} columns\")\n",
        "\n",
        "\n",
        "# 11. Save to CSV\n",
        "output_path = '/content/drive/MyDrive/cleaned_meridian.csv'\n",
        "try:\n",
        "    pivot_table.to_csv(output_path, index=False)\n",
        "    print(f\"\\nCleaned data saved to {output_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError saving cleaned data to {output_path}: {e}\")"
      ],
      "metadata": {
        "id": "S_xS5XKdPf1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uc9f24qtP0lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JBDZzl80BrY"
      },
      "source": [
        "2\\. Map the media variables and the media spends to the designated channel names intended for display in the two-page HTML output. In the following example,  'Channel0_impression' and 'Channel0_spend' are connected to the same channel, 'Channel0'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qdTSk4a0znn"
      },
      "outputs": [],
      "source": [
        "coord_to_columns = load.CoordToColumns(\n",
        "    time='Week',\n",
        "    geo='Region_(Matched)',\n",
        "    controls=['Brand_GQV', 'Generic_GQV'],  # Updated to use available GQV columns\n",
        "    population='Population',\n",
        "    kpi='Total_Conversions',\n",
        "    revenue_per_kpi='Revenue',\n",
        "    media=[\n",
        "        'Search_Impression',\n",
        "        'Performance_Max_Impression', # Corrected to match cleaned data naming\n",
        "        'Display_Impression',\n",
        "        'Demand_Gen_Impression', # Corrected to match cleaned data naming\n",
        "        'Facebook_Impression',\n",
        "        'TikTok_Impression',\n",
        "        'YouTube_Impression',\n",
        "        'Display_Plug_Impression',\n",
        "        'Display_STB_Impression',\n",
        "        'Facebook_Awareness_Impression',\n",
        "        'Facebook_Consideration_Impression',\n",
        "        'Facebook_Conversion_Impression',\n",
        "        'Facebook_Share_to_Buy_Impression',\n",
        "\n",
        "    ],\n",
        "    media_spend=[\n",
        "        'Google_Search_Media_Cost', # Corrected to match cleaned data naming\n",
        "        'Google_Performance_Max_Media_Cost', # Corrected to match cleaned data naming\n",
        "        'Display_Media_Cost',\n",
        "        'Google_Demand_Gen_Media_Cost', # Corrected to match cleaned data naming\n",
        "        'Facebook_Media_Cost',\n",
        "        'TikTok_Media_Cost',\n",
        "        'YouTube_Media_Cost',\n",
        "        'Display_Plug_Media_Cost',\n",
        "        'Display_STB_Media_Cost',\n",
        "        'Facebook_Awareness_Media_Cost',\n",
        "        'Facebook_Consideration_Media_Cost',\n",
        "        'Facebook_Conversion_Media_Cost',\n",
        "        'Facebook_Share_to_Buy_Media_Cost',\n",
        "    ]\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_media_to_channel = {\n",
        "    'Search_Impression': 'Search',\n",
        "    'Performance_Max_Impression': 'Performance_Max', # Corrected to match coord_to_columns\n",
        "    'Display_Impression': 'Display',\n",
        "    'Demand_Gen_Impression': 'Demand_Gen', # Corrected to match coord_to_columns\n",
        "    'Facebook_Impression': 'Facebook',\n",
        "    'TikTok_Impression': 'TikTok',\n",
        "    'YouTube_Impression': 'YouTube',\n",
        "    'Display_Plug_Impression': 'Display_Plug',\n",
        "    'Display_STB_Impression': 'Display_STB',\n",
        "    'Facebook_Awareness_Impression': 'Facebook_Awareness',\n",
        "    'Facebook_Consideration_Impression': 'Facebook_Consideration',\n",
        "    'Facebook_Conversion_Impression': 'Facebook_Conversion',\n",
        "    'Facebook_Share_to_Buy_Impression': 'Facebook_Share_to_Buy',\n",
        "}\n",
        "correct_media_spend_to_channel = {\n",
        "    'Google_Search_Media_Cost': 'Search', # Corrected to match cleaned data naming and channel\n",
        "    'Google_Performance_Max_Media_Cost': 'Performance_Max', # Corrected to match cleaned data naming and channel\n",
        "    'Display_Media_Cost': 'Display',\n",
        "    'Google_Demand_Gen_Media_Cost': 'Demand_Gen', # Corrected to match cleaned data naming and channel\n",
        "    'Facebook_Media_Cost': 'Facebook',\n",
        "    'TikTok_Media_Cost': 'TikTok',\n",
        "    'YouTube_Media_Cost': 'YouTube',\n",
        "    'Display_Plug_Media_Cost': 'Display_Plug',\n",
        "    'Display_STB_Media_Cost': 'Display_STB',\n",
        "    'Facebook_Awareness_Media_Cost': 'Facebook_Awareness',\n",
        "    'Facebook_Consideration_Media_Cost': 'Facebook_Consideration',\n",
        "    'Facebook_Conversion_Media_Cost': 'Facebook_Conversion',\n",
        "    'Facebook_Share_to_Buy_Media_Cost': 'Facebook_Share_to_Buy',\n",
        "}"
      ],
      "metadata": {
        "id": "bLzbY6EQP2_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNr75vQL1Zru"
      },
      "source": [
        "3\\. Load the CSV data using `CsvDataLoader`. Note that `csv_path` is the path to the data file location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udaLGvwl1U8B"
      },
      "outputs": [],
      "source": [
        "# Add diagnostic print statements\n",
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned data to inspect columns\n",
        "cleaned_df = pd.read_csv('/content/drive/MyDrive/cleaned_meridian.csv')\n",
        "print(\"Columns in cleaned_meridian.csv:\", cleaned_df.columns.tolist())\n",
        "\n",
        "# Get the media_spend columns from the cleaned data\n",
        "cleaned_media_spend_cols = [col for col in cleaned_df.columns if col in coord_to_columns.media_spend]\n",
        "print(\"Media spend columns found in cleaned data (matching coord_to_columns):\", cleaned_media_spend_cols)\n",
        "\n",
        "# Get the keys from correct_media_spend_to_channel\n",
        "media_spend_channel_keys = list(correct_media_spend_to_channel.keys())\n",
        "print(\"Keys in correct_media_spend_to_channel:\", media_spend_channel_keys)\n",
        "\n",
        "# Get the media_spend list from coord_to_columns\n",
        "coord_media_spend_list = coord_to_columns.media_spend\n",
        "print(\"media_spend list in coord_to_columns:\", coord_media_spend_list)\n",
        "\n",
        "# Compare the sets of columns/keys\n",
        "print(\"Are media_spend columns in cleaned data equal to media_spend list in coord_to_columns?\", set(cleaned_media_spend_cols) == set(coord_media_spend_list))\n",
        "print(\"Are media_spend list in coord_to_columns equal to keys in correct_media_spend_to_channel?\", set(coord_media_spend_list) == set(media_spend_channel_keys))\n",
        "\n",
        "\n",
        "loader = load.CsvDataLoader(\n",
        "    csv_path='/content/drive/MyDrive/cleaned_meridian.csv',\n",
        "    kpi_type='non_revenue',\n",
        "    coord_to_columns=coord_to_columns,\n",
        "    media_to_channel=correct_media_to_channel,\n",
        "    media_spend_to_channel=correct_media_spend_to_channel,\n",
        ")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlF5vs8vb8Wn"
      },
      "source": [
        "Note that the simulated data here contains reach and frequency channels. We recommend including reach and frequency data whenever they are available. For information about the advantages of utilizing reach and frequency, see [Bayesian Hierarchical Media Mix Model Incorporating Reach and Frequency Data](https://research.google/pubs/bayesian-hierarchical-media-mix-model-incorporating-reach-and-frequency-data/#:~:text=By%20incorporating%20R%26F%20into%20MMM,based%20on%20optimal%20frequency%20recommendations.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO6pDd6f2V1L"
      },
      "source": [
        "<a name=\"configure-model\"></a>\n",
        "## Step 2: Configure the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_mQI7HzxxK4"
      },
      "source": [
        "Meridian uses Bayesian framework and Markov Chain Monte Carlo (MCMC) algorithms to sample from the posterior distribution.\n",
        "\n",
        "1\\. Inititalize the `Meridian` class by passing the loaded data and the customized model specification. One advantage of Meridian lies in its capacity to calibrate the model directly through ROI priors, as described in [Media Mix Model Calibration With Bayesian Priors](https://research.google/pubs/media-mix-model-calibration-with-bayesian-priors/). In this particular example, the ROI priors for all media channels are identical, with each being represented as Lognormal(0.2, 0.9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XNDd7HX1qTn"
      },
      "outputs": [],
      "source": [
        "roi_rf_mu = 0.2     # Mu for ROI prior for each RF channel.\n",
        "roi_rf_sigma = 0.9  # Sigma for ROI prior for each RF channel.\n",
        "prior = prior_distribution.PriorDistribution(\n",
        "    roi_rf=tfp.distributions.LogNormal(roi_rf_mu, roi_rf_sigma, name=constants.ROI_RF)\n",
        ")\n",
        "model_spec = spec.ModelSpec(prior=prior)\n",
        "\n",
        "mmm = model.Meridian(input_data=data, model_spec=model_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPQBPlX8cmEv"
      },
      "source": [
        "2\\. Use the `sample_prior()` and `sample_posterior()` methods to obtain samples from the prior and posterior distributions of model parameters. If you are using the T4 GPU runtime this step may take about 10 minutes for the provided data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVB3avRdcRNz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "mmm.sample_prior(500)\n",
        "mmm.sample_posterior(n_chains=10, n_adapt=2000, n_burnin=500, n_keep=500, seed=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUM2V26cspo"
      },
      "source": [
        "For more information about configuring the parameters and using a customized model specification, such as setting different ROI priors for each media channel, see [Configure the model](https://developers.google.com/meridian/docs/user-guide/configure-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9oECJwUdJTm"
      },
      "source": [
        "<a name=\"model-diagnostics\"></a>\n",
        "## Step 3: Run model diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSzK6JeMxrV6"
      },
      "source": [
        "After the model is built, you must assess convergence, debug the model if needed, and then assess the model fit.\n",
        "\n",
        "1\\. Assess convergence. Run the following code to generate r-hat statistics. R-hat close to 1.0 indicate convergence. R-hat < 1.2 indicates approximate convergence and is a reasonable threshold for many problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFuc7B86yLvM"
      },
      "outputs": [],
      "source": [
        "model_diagnostics = visualizer.ModelDiagnostics(mmm)\n",
        "model_diagnostics.plot_rhat_boxplot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCwt5SGYxlaE"
      },
      "source": [
        "2\\. Assess the model's fit by comparing the expected sales against the actual sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z4zJtHyyhif"
      },
      "outputs": [],
      "source": [
        "model_fit = visualizer.ModelFit(mmm)\n",
        "model_fit.plot_model_fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76IBQcWLu980"
      },
      "source": [
        "For more information and additional model diagnostics checks, see [Modeling diagnostics](https://developers.google.com/meridian/docs/user-guide/model-diagnostics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGUOFFbCdOtl"
      },
      "source": [
        "<a name=\"generate-summary\"></a>\n",
        "## Step 4: Generate model results & two-page output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puHjkyvZEOEg"
      },
      "source": [
        "To export the two-page HTML summary output, initialize the `Summarizer` class with the model object. Then pass in the filename, filepath, start date, and end date to `output_model_results_summary` to run the summary for that time duration and save it to the specified file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keOpq1qKNbq0"
      },
      "outputs": [],
      "source": [
        "mmm_summarizer = summarizer.Summarizer(mmm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ltr4uP80YQe7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbgNaDYpIfQl"
      },
      "outputs": [],
      "source": [
        "filepath = '/content/drive/MyDrive'\n",
        "start_date = '2021-01-25'\n",
        "end_date = '2025-01-15'\n",
        "mmm_summarizer.output_model_results_summary('summary_output.html', filepath, start_date, end_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9sBxuvidmr8"
      },
      "source": [
        "Here is a preview of the two-page output based on the simulated data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaUe7uZRfJPm"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(filename='/content/drive/MyDrive/summary_output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PphWMfKdwPIw"
      },
      "source": [
        "For a customized two-page report, model results summary table, and individual visualizations, see [Model results report](https://developers.google.com/meridian/docs/user-guide/generate-model-results-report) and [plot media visualizations](https://developers.google.com/meridian/docs/user-guide/plot-media-visualizations).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msqwz2MN5mTq"
      },
      "source": [
        "<a name=\"generate-optimize\"></a>\n",
        "## Step 5: Run budget optimization & generate an optimization report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khCL6Q2sS-iy"
      },
      "source": [
        "You can choose what scenario to run for the budget allocation. In default scenario, you find the optimal allocation across channels for a given budget to maximize the return on investment (ROI).\n",
        "\n",
        "1\\. Instantiate the `BudgetOptimizer` class and run the `optimize()` method without any customization, to run the default library's Fixed Budget Scenario to maximize ROI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38lhqyLvHf51"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "budget_optimizer = optimizer.BudgetOptimizer(mmm)\n",
        "optimization_results = budget_optimizer.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLOMqDmCRKRO"
      },
      "source": [
        "2\\. Export the 2-page HTML optimization report, which contains optimized spend allocations and ROI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at7V7YEh_zwZ"
      },
      "outputs": [],
      "source": [
        "filepath = '/content/drive/MyDrive'\n",
        "optimization_results.output_optimization_summary('optimization_output.html', filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq_mcrj1STDU"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(filename='/content/drive/MyDrive/optimization_output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIWTubaN0RKC"
      },
      "source": [
        "For information about customized optimization scenarios, such as flexible budget scenarios, see [Budget optimization scenarios](https://developers.google.com/meridian/docs/user-guide/budget-optimization-scenarios). For more information about optimization results summary and individual visualizations, see [optimization results output](https://developers.google.com/meridian/docs/user-guide/generate-optimization-results-output) and [optimization visualizations](https://developers.google.com/meridian/docs/user-guide/plot-optimization-visualizations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m98O3a_TrVg"
      },
      "source": [
        "<a name=\"save-model\"></a>\n",
        "## Step 6: Save the model object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zjh64YG8Dti"
      },
      "source": [
        "We recommend that you save the model object for future use. This helps you to  avoid repetitive model runs and saves time and computational resources. After the model object is saved, you can load it at a later stage to continue the analysis or visualizations without having to re-run the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kamZpyv8KMh"
      },
      "source": [
        "Run the following codes to save the model object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfaQQ8-fTw0K"
      },
      "outputs": [],
      "source": [
        "file_path='/content/drive/MyDrive/saved_mmm.pkl'\n",
        "model.save_mmm(mmm, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2v_s2uS8PgA"
      },
      "source": [
        "Run the following codes to load the saved model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGUmiYI48epA"
      },
      "outputs": [],
      "source": [
        "mmm = model.load_mmm(file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}